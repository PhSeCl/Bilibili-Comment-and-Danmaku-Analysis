import streamlit as st
import sys
import os
from pathlib import Path
import pandas as pd
import time

# Page Config
st.set_page_config(
    page_title="Bilibili Comment Analysis",
    page_icon="ğŸ“Š",
    layout="wide"
)

# Add project root to sys.path
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.append(str(PROJECT_ROOT))

# Title
st.title("ğŸ“º Bilibili è¯„è®ºä¸å¼¹å¹•æƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
st.markdown("é¡¹ç›®å¼€æºåœ°å€: [GitHub](https://github.com/PhSeCl/Bilibili-Comments-and-Danmaku-Analysis)")
st.markdown("---")

# Sidebar: Configuration
st.sidebar.header("âš™ï¸ å‚æ•°è®¾ç½®")

# Cookie è¾“å…¥æ¡†
user_cookie = st.sidebar.text_area(
    "Bç«™ Cookie (å¯é€‰ï¼Œç”¨äºçˆ¬å–æ›´å¤šæ•°æ®)", 
    value="",
    placeholder="åœ¨æ­¤å¯ç²˜è´´æ‚¨çš„ Cookieï¼Œç•™ç©ºåˆ™ä½¿ç”¨é»˜è®¤æµ‹è¯• Cookie",
    help="ç™»å½• B ç«™åï¼ŒæŒ‰ F12 æ‰“å¼€æ§åˆ¶å°ï¼Œè¾“å…¥ document.cookie å¹¶å¤åˆ¶ç»“æœã€‚"
)

# æ›´æ–° config ä¸­çš„ Cookie
from src.crawler import config
if user_cookie.strip():
    config.COOKIE = user_cookie.strip()
    config.HEADERS["Cookie"] = config.COOKIE
else:
    # å¦‚æœç”¨æˆ·æœªè¾“å…¥ï¼Œä½¿ç”¨é»˜è®¤ Cookie
    if hasattr(config, 'DEFAULT_COOKIE'):
        config.COOKIE = config.DEFAULT_COOKIE
        config.HEADERS["Cookie"] = config.COOKIE

bv_code = st.sidebar.text_input("BV å· (ä¾‹å¦‚ BV1xx411c7mD)", value="BV1xx411c7mD")
max_pages = st.sidebar.number_input("çˆ¬å–é¡µæ•° (æ¯é¡µ20æ¡)", min_value=1, max_value=100, value=5)
max_danmaku = st.sidebar.number_input("å¼¹å¹•çˆ¬å–æ¡æ•° (0ä¸ºä¸é™åˆ¶)", min_value=0, value=1000, step=100)

st.sidebar.markdown("---")
st.sidebar.info("æç¤ºï¼šå…ˆçˆ¬å–æ•°æ®ï¼Œå†è¿›è¡Œåˆ†æã€‚")

# --- å¯åŠ¨åŠ è½½åŠ¨ç”» ---
loading_placeholder = st.empty()
with loading_placeholder.container():
    # åˆ›å»ºä¸‰åˆ—å¸ƒå±€è®©å†…å®¹å±…ä¸­
    l_col1, l_col2, l_col3 = st.columns([1, 2, 1])
    with l_col2:
        st.markdown("<br><br>", unsafe_allow_html=True) # é¡¶å‡ºä¸€ç‚¹è·ç¦»
        st.markdown("<h2 style='text-align: center;'>å°‘å¥³æŠ˜å¯¿ä¸­... ğŸ™</h2>", unsafe_allow_html=True)
        
        # å°è¯•åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰å›¾ç‰‡
        image_path = PROJECT_ROOT / "assets" / "loading.png"
        if image_path.exists():
            st.image(str(image_path), use_container_width=True)
        else:
            # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œæ˜¾ç¤ºä¸€ä¸ªæç¤ºæˆ–è€… emoji
            st.markdown("<div style='text-align: center; font-size: 80px;'>ğŸ›</div>", unsafe_allow_html=True)
            st.caption("ï¼ˆæç¤ºï¼šæ‚¨å¯ä»¥å°†Qç‰ˆå›¾ç‰‡å‘½åä¸º loading.png å¹¶æ”¾å…¥ assets æ–‡ä»¶å¤¹ä¸­ï¼‰")

# Import project modules
try:
    from src.crawler.main_crawler import crawl_comments_by_bv, crawl_danmaku_by_bv, get_video_info
    from src.analysis.run_prediction import run_prediction_pipeline
    from src.visualization.distribution import plot_emotion_distribution
    from src.visualization.timeline import plot_comment_timeline, plot_video_progress_trend
    from src.visualization.viz_geo_heatmap import plot_geo_heatmap
    from src.visualization.wordcloud_viz import generate_wordcloud
    
    # æ¨¡æ‹Ÿä¸€ç‚¹å»¶è¿Ÿï¼Œè®©ç”¨æˆ·èƒ½çœ‹æ¸…åŠ¨ç”» (å¯é€‰ï¼Œå¦‚æœåŠ è½½å¤ªå¿«çš„è¯)
    # time.sleep(1) 
    
except ImportError as e:
    st.error(f"Import Error: {e}")
    st.stop()

# --- ç¼“å­˜æ¨¡å‹åŠ è½½ ---
@st.cache_resource
def load_sentiment_model():
    """
    åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹å¹¶ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    
    LOCAL_MODEL_DIR = PROJECT_ROOT / "trained_models"
    HF_MODEL_ID = "ScarletShinku/bilibili-sentiment-bert"
    
    model_path = LOCAL_MODEL_DIR if LOCAL_MODEL_DIR.exists() else HF_MODEL_ID
    
    print(f"ğŸš€ [Cache] Loading model from: {model_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

# æ¸…é™¤åŠ è½½åŠ¨ç”»
loading_placeholder.empty()

# Main Content
col1, col2 = st.columns(2)

with col1:
    st.subheader("1. æ•°æ®é‡‡é›†")
    
    # Tabs for Comments and Danmaku
    crawl_tab1, crawl_tab2 = st.tabs(["ğŸ“ è¯„è®º", "ğŸš€ å¼¹å¹•"])
    
    with crawl_tab1:
        if st.button("ğŸ•·ï¸ å¼€å§‹çˆ¬å–è¯„è®º", use_container_width=True):
            if not bv_code:
                st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ BV å·")
            else:
                with st.spinner(f"æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯: {bv_code}..."):
                    video_info = get_video_info(bv_code)
                    
                if not video_info:
                    st.error("æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ BV å·æˆ–ç½‘ç»œã€‚")
                else:
                    st.success(f"æ‰¾åˆ°è§†é¢‘ (OID: {video_info['oid']})")
                    
                    # Progress bar
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # Define a custom output path for this session
                    raw_data_path = PROJECT_ROOT / "data" / "raw" / f"comments_{bv_code}.csv"
                    
                    # Callback function for progress
                    def progress_callback(current, total, msg):
                        status_text.text(msg)
                        if total > 0:
                            progress_bar.progress(min(current / total, 1.0))
                    
                    # Run crawler
                    try:
                        count = crawl_comments_by_bv(bv_code, max_pages, str(raw_data_path), callback=progress_callback)
                        progress_bar.progress(100)
                        if count > 0:
                            st.success(f"âœ… çˆ¬å–å®Œæˆï¼å…±è·å– {count} æ¡è¯„è®ºã€‚")
                            st.session_state['current_raw_data'] = str(raw_data_path)
                            st.session_state['current_bv'] = bv_code
                        else:
                            st.warning("âš ï¸ æœªçˆ¬å–åˆ°ä»»ä½•è¯„è®ºã€‚")
                    except Exception as e:
                        st.error(f"çˆ¬å–å¤±è´¥: {e}")

    with crawl_tab2:
        if st.button("ğŸš€ å¼€å§‹çˆ¬å–å¼¹å¹•", use_container_width=True):
            if not bv_code:
                st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„ BV å·")
            else:
                with st.spinner(f"æ­£åœ¨çˆ¬å–å¼¹å¹•: {bv_code}..."):
                    danmaku_path = PROJECT_ROOT / "data" / "raw" / f"danmaku_{bv_code}.csv"
                    limit = max_danmaku if max_danmaku > 0 else None
                    try:
                        count = crawl_danmaku_by_bv(bv_code, limit, str(danmaku_path))
                        if count > 0:
                            st.success(f"âœ… å¼¹å¹•çˆ¬å–å®Œæˆï¼å…± {count} æ¡ã€‚")
                            st.info(f"ä¿å­˜è·¯å¾„: {danmaku_path.name}")
                        else:
                            st.warning("âš ï¸ æœªçˆ¬å–åˆ°å¼¹å¹•ã€‚")
                    except Exception as e:
                        st.error(f"å¼¹å¹•çˆ¬å–å¤±è´¥: {e}")

with col2:
    st.subheader("2. æƒ…æ„Ÿåˆ†æ")
    
    # File Selection Logic
    raw_data_dir = PROJECT_ROOT / "data" / "raw"
    if raw_data_dir.exists():
        csv_files = list(raw_data_dir.glob("*.csv"))
        # Sort by modification time (newest first)
        csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        file_options = {f.name: str(f) for f in csv_files}
    else:
        file_options = {}

    selected_file_name = st.selectbox(
        "é€‰æ‹©è¦åˆ†æçš„æ•°æ®æ–‡ä»¶:",
        options=list(file_options.keys()),
        index=0 if file_options else None,
        help="ä» data/raw ç›®å½•ä¸­é€‰æ‹©å·²çˆ¬å–çš„ CSV æ–‡ä»¶"
    )
    
    current_raw_data = file_options.get(selected_file_name) if selected_file_name else None

    if st.button("ğŸ§  å¼€å§‹åˆ†æ", disabled=not current_raw_data, use_container_width=True):
        with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹å¹¶åˆ†ææƒ…æ„Ÿ (å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)..."):
            try:
                # é¢„åŠ è½½æ¨¡å‹
                model, tokenizer = load_sentiment_model()
                if model is None:
                    st.error("æ— æ³•åŠ è½½æ¨¡å‹ï¼Œåˆ†æç»ˆæ­¢ã€‚")
                else:
                    output_csv = PROJECT_ROOT / "data" / "processed" / f"predictions_{Path(current_raw_data).stem}.csv"
                    # ä¼ å…¥é¢„åŠ è½½çš„æ¨¡å‹
                    df = run_prediction_pipeline(
                        input_path=current_raw_data, 
                        output_path=output_csv,
                        model=model,
                        tokenizer=tokenizer
                    )
                    
                    if df is not None:
                        st.session_state['analysis_result'] = df
                        st.success("âœ… åˆ†æå®Œæˆï¼")
                    else:
                        st.error("åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            except Exception as e:
                st.error(f"è¿è¡Œå‡ºé”™: {e}")

st.markdown("---")

# Visualization Section
if 'analysis_result' in st.session_state:
    df = st.session_state['analysis_result']
    
    st.header("ğŸ“Š åˆ†æç»“æœå¯è§†åŒ–")
    
    # åŠ¨æ€ç”Ÿæˆæ ‡ç­¾é¡µ
    tab_names = ["æƒ…æ„Ÿåˆ†å¸ƒ", "æ—¶é—´è¶‹åŠ¿", "è¯äº‘å›¾"]
    has_location = 'ip_location' in df.columns
    if has_location:
        tab_names.append("åœ°åŸŸçƒ­åŠ›å›¾")
    tab_names.append("åŸå§‹æ•°æ®")
    
    tabs = st.tabs(tab_names)
    
    tab1 = tabs[0]
    tab2 = tabs[1]
    tab_wc = tabs[2]
    
    if has_location:
        tab3 = tabs[3]
        tab4 = tabs[4]
    else:
        tab3 = None
        tab4 = tabs[3]
    
    with tab1:
        st.subheader("æ€»ä½“æƒ…æ„Ÿåˆ†å¸ƒ")
        try:
            fig, _ = plot_emotion_distribution(df, save_path=None)
            st.pyplot(fig)
        except Exception as e:
            st.error(f"ç»˜å›¾å¤±è´¥: {e}")
            
    with tab2:
        st.subheader("æƒ…æ„Ÿéšæ—¶é—´å˜åŒ–")
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå¼¹å¹•æ•°æ® (å«æœ‰ video_time åˆ—)
        is_danmaku = 'video_time' in df.columns
        
        if is_danmaku:
            timeline_mode = st.radio("æ—¶é—´ç»´åº¦:", ["ç°å®æ—¶é—´ (å‘å¸ƒæ—¥æœŸ)", "è§†é¢‘è¿›åº¦ (æ’­æ”¾æ—¶é—´)"], horizontal=True)
        else:
            timeline_mode = "ç°å®æ—¶é—´ (å‘å¸ƒæ—¥æœŸ)"
            
        if timeline_mode == "è§†é¢‘è¿›åº¦ (æ’­æ”¾æ—¶é—´)":
             try:
                # åˆ†ç®±å¤§å°æ»‘å—
                bin_size = st.slider("æ—¶é—´åˆ†ç®±å¤§å° (ç§’)", min_value=10, max_value=300, value=30, step=10)
                fig_timeline, _ = plot_video_progress_trend(df, time_column='video_time', bin_size=bin_size)
                if fig_timeline:
                    st.pyplot(fig_timeline)
                else:
                    st.info("æ•°æ®ä¸è¶³ä»¥ç”Ÿæˆè§†é¢‘è¿›åº¦å›¾ã€‚")
             except Exception as e:
                st.error(f"è§†é¢‘è¿›åº¦ç»˜å›¾å¤±è´¥: {e}")
        
        else:
            # å°è¯•æŸ¥æ‰¾æ—¶é—´åˆ—
            date_col = None
            if 'time' in df.columns:
                date_col = 'time'
            elif 'date' in df.columns:
                date_col = 'date'
            elif 'real_time' in df.columns:
                date_col = 'real_time'
                
            if date_col:
                try:
                    # Convert to datetime if needed
                    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
                    
                    # æ—¶é—´é¢‘ç‡é€‰æ‹©
                    freq_map = {
                        "æŒ‰å°æ—¶": "H", 
                        "æŒ‰å¤©": "D", 
                        "æ¯3å¤©": "3D", 
                        "æŒ‰å‘¨": "W", 
                        "æ¯åŠæœˆ": "15D", 
                        "æŒ‰æœˆ": "M"
                    }
                    freq_label = st.select_slider(
                        "æ—¶é—´èšåˆç²’åº¦:", 
                        options=list(freq_map.keys()), 
                        value="æŒ‰å¤©"
                    )
                    freq = freq_map[freq_label]
                    
                    fig_timeline, _ = plot_comment_timeline(df, date_column=date_col, freq=freq)
                    if fig_timeline:
                        st.pyplot(fig_timeline)
                    else:
                        st.info("æ•°æ®ä¸è¶³ä»¥ç”Ÿæˆæ—¶é—´åºåˆ—å›¾ã€‚")
                except Exception as e:
                    st.error(f"æ—¶é—´åºåˆ—ç»˜å›¾å¤±è´¥: {e}")
            else:
                st.warning("æ•°æ®ä¸­ç¼ºå°‘æ—¶é—´åˆ— (time/date/real_time)ï¼Œæ— æ³•ç»˜åˆ¶è¶‹åŠ¿å›¾ã€‚")
    
    with tab_wc:
        st.subheader("è¯„è®ºè¯äº‘å›¾")
        st.info("è¯äº‘å›¾å±•ç¤ºäº†è¯„è®ºä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„è¯æ±‡ã€‚")
        try:
            fig_wc = generate_wordcloud(df)
            
            if fig_wc:
                st.pyplot(fig_wc)
            else:
                st.warning("æ— æ³•ç”Ÿæˆè¯äº‘ï¼ˆå¯èƒ½è¯„è®ºå¤ªå°‘æˆ–ç¼ºå°‘ä¾èµ–ï¼‰")
                    
        except Exception as e:
            st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {e}")
            st.warning("æç¤º: è¯·ç¡®ä¿å·²å®‰è£… jieba å’Œ wordcloud åº“ã€‚")
            
    if tab3:
        with tab3:
            st.subheader("è¯„è®ºç”¨æˆ·åœ°åŸŸåˆ†å¸ƒ")
            
            # ç»Ÿè®¡å¹¶å±•ç¤ºæ— åœ°åŸŸä¿¡æ¯çš„è¯„è®º
            total_count = len(df)
            # ç­›é€‰å‡ºæ— æ•ˆçš„åœ°ç†ä½ç½® (NaN, ç©ºå­—ç¬¦ä¸², æˆ– "æœªçŸ¥")
            unknown_mask = (
                df['ip_location'].isna() | 
                (df['ip_location'].astype(str).str.strip() == '') | 
                (df['ip_location'] == 'æœªçŸ¥')
            )
            unknown_count = unknown_mask.sum()
            
            # è®¡ç®—ç¼ºå¤±ç‡
            unknown_ratio = unknown_count / total_count if total_count > 0 else 0
            
            # å¦‚æœç¼ºå¤±ç‡è¿‡é«˜ (ä¾‹å¦‚è¶…è¿‡ 90%)ï¼Œåˆ™æ‹’ç»ç”Ÿæˆ
            if unknown_ratio > 0.9:
                st.error(f"âš ï¸ æ— æ³•ç”Ÿæˆçƒ­åŠ›å›¾ï¼šæ•°æ®ä¸­ {unknown_ratio:.1%} çš„è¯„è®ºç¼ºå°‘IPå±åœ°ä¿¡æ¯ï¼ˆè¯„è®ºæ—¶é—´è¿‡äºå¤è€ï¼‰ï¼Œæœ‰æ•ˆæ ·æœ¬è¿‡å°‘ã€‚")
            else:
                if unknown_count > 0:
                    st.info(f"â„¹ï¸ æ•°æ®è¯´æ˜ï¼šå…±æœ‰ {total_count} æ¡è¯„è®ºï¼Œå…¶ä¸­ {unknown_count} æ¡ ({unknown_ratio:.1%}) æœªæ˜¾ç¤ºIPå±åœ°ï¼Œå·²åœ¨åœ°å›¾ä¸­æ’é™¤ã€‚")
            
                heatmap_mode = st.radio("æ˜¾ç¤ºæ¨¡å¼:", ["è¯„è®ºæ•°é‡", "æƒ…æ„Ÿå€¾å‘"], horizontal=True)
                mode_key = 'sentiment' if heatmap_mode == "æƒ…æ„Ÿå€¾å‘" else 'count'
                
                try:
                    # Use a temporary file for the HTML output
                    temp_html = PROJECT_ROOT / "docs" / "temp_heatmap.html"
                    # Pass the DataFrame directly instead of path, and pass the mode
                    c = plot_geo_heatmap(df, str(temp_html), mode=mode_key)
                    if c:
                        # Render HTML in Streamlit
                        import streamlit.components.v1 as components
                        with open(temp_html, 'r', encoding='utf-8') as f:
                            html_content = f.read()
                        components.html(html_content, height=600)
                    else:
                        st.warning("æ— æ³•ç”Ÿæˆçƒ­åŠ›å›¾ã€‚")
                except Exception as e:
                    st.error(f"çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")

    with tab4:
        st.subheader("è¯„è®ºæ•°æ®é¢„è§ˆ")
        st.dataframe(df[['content', 'labels', 'time']].head(100) if 'time' in df.columns else df[['content', 'labels']].head(100))
        
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å®Œæ•´åˆ†æç»“æœ (CSV)",
            data=csv,
            file_name=f"sentiment_analysis_{st.session_state.get('current_bv', 'result')}.csv",
            mime='text/csv',
        )
