import requests
import time
import csv
import random
import re
import os
import json
import config 

def get_video_info(bv):
    """é€šè¿‡BVå·è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯ (OID, CID, æ ‡é¢˜, ä½œè€…, æ—¶é—´, æ ‡ç­¾)"""
    url = f"https://www.bilibili.com/video/{bv}"
    # æ—¢ç„¶è¦æŠ“è¯¦ç»†ä¿¡æ¯ï¼Œä¸å¦‚ç›´æ¥è°ƒ APIï¼Œæ¯”æ­£åˆ™çˆ¬ç½‘é¡µæ›´ç¨³
    api_url = "https://api.bilibili.com/x/web-interface/view"
    params = {"bvid": bv}
    
    try:
        resp = requests.get(api_url, params=params, headers=config.HEADERS)
        data = resp.json()
        
        if data['code'] == 0:
            info = data['data']
            # æå–æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯
            return {
                "oid": info['aid'],
                "cid": info['cid'],
                "title": info['title'],
                "owner": info['owner']['name'],
                "pubdate": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(info['pubdate'])),
                "tags": [tag['tag_name'] for tag in info.get('tags', [])] if 'tags' in info else []
                # æ³¨æ„ï¼šAPI v2 æœ‰æ—¶ tags ä¸åœ¨è¿™é‡Œï¼Œå¦‚æœä¸é‡è¦å¯ä»¥å…ˆå¿½ç•¥ï¼Œæˆ–è€…è°ƒå¦ä¸€ä¸ª tag æ¥å£
                # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœæ²¡æœ‰ tags å­—æ®µå°±ç•™ç©º
            }
        else:
            print(f"âŒ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {data['message']}")
            return None
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None

def write_metadata(filename, video_info):
    """åœ¨CSVæ–‡ä»¶å¼€å¤´å†™å…¥è§†é¢‘å…ƒæ•°æ®"""
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # ä¸ºäº†ä¸ç ´å CSV ç»“æ„ï¼Œæˆ‘ä»¬æŠŠå…ƒæ•°æ®å†™æˆæ³¨é‡Šå½¢å¼ (# å¼€å¤´)
    # æˆ–è€…ç›´æ¥å†™åœ¨å‰å‡ è¡Œï¼Œåç»­è¯»å–æ—¶è·³è¿‡
    with open(filename, mode='w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["# è§†é¢‘æ ‡é¢˜", video_info['title']])
        writer.writerow(["# BVå·", config.BV_CODE]) # è¿™é‡Œä¹Ÿå¯ä»¥ç”¨å‚æ•°ä¼ è¿›æ¥çš„ bv
        writer.writerow(["# ä½œè€…", video_info['owner']])
        writer.writerow(["# å‘å¸ƒæ—¶é—´", video_info['pubdate']])
        # æ ‡ç­¾å¯èƒ½å«æœ‰é€—å·ï¼Œä¸ºäº†é˜²æ­¢ CSV é”™ä½ï¼Œæ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        tags_str = "|".join(video_info['tags']) if video_info['tags'] else "æ— æ ‡ç­¾"
        writer.writerow(["# æ ‡ç­¾", tags_str])
        writer.writerow([]) # ç©ºä¸€è¡Œï¼Œç¾è§‚
        # è¿™é‡Œä¸å†™è¡¨å¤´ï¼Œè¡¨å¤´ç•™ç»™åé¢çš„å‡½æ•°å†™

# ==================== è¯„è®ºçˆ¬å–éƒ¨åˆ† ====================
def fetch_comments(oid, page):
    url = "https://api.bilibili.com/x/v2/reply"
    params = {
        "type": 1,
        "oid": oid,
        "sort": 2, # å¦‚æœè¦çˆ¬å…¨éƒ¨ï¼Œå»ºè®®è¿˜æ˜¯ç”¨ 2 (æŒ‰æ—¶é—´)ï¼›å¦‚æœå¤±æ•ˆäº†å†æ”¹ 0
        "pn": page,
        "ps": 20
    }
    try:
        resp = requests.get(url, params=params, headers=config.HEADERS)
        data = resp.json()
        if data['code'] == 0:
            return data['data']['replies']
        return None
    except Exception as e:
        print(f"âŒ è·å–è¯„è®ºç¬¬ {page} é¡µå¤±è´¥: {e}")
        return None

def save_comments_to_csv(comments, filename, is_first_write=False):
    """ä¿å­˜è¯„è®º"""
    # å®šä¹‰ CSV è¡¨å¤´
    headers = ['rpid', 'username', 'level', 'content', 'likes', 'location', 'date']
    
    with open(filename, mode='a', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        # åªæœ‰åœ¨ç¬¬ä¸€æ¬¡å†™å…¥æ•°æ®æ—¶ï¼Œæ‰å†™è¡¨å¤´
        if is_first_write:
            writer.writerow(headers)
        
        count = 0
        if not comments: return 0
        
        for c in comments:
            if not c: continue
            try:
                rpid = c['rpid']
                uname = c['member']['uname']
                content = c['content']['message']
                likes = c['like']
                ctime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(c['ctime']))
                level = c['member']['level_info']['current_level']
                raw_location = c.get('reply_control', {}).get('location', '')
                location = raw_location.replace("IPå±åœ°ï¼š", "") if raw_location else "æœªçŸ¥"
                
                writer.writerow([rpid, uname, level, content, likes, location, ctime])
                count += 1
            except Exception as e:
                print(f"âš ï¸ è§£æå¤±è´¥: {e} | æ•°æ®ç‰‡æ®µ: {str(c)[:50]}...") 
                continue
        return count

# ==================== å¼¹å¹•çˆ¬å–éƒ¨åˆ† ====================
def crawl_danmaku_xml(cid):
    url = f"https://comment.bilibili.com/{cid}.xml"
    try:
        resp = requests.get(url, headers=config.HEADERS)
        resp.encoding = 'utf-8'
        patterns = re.findall(r'<d p="([^"]+)">([^<]+)</d>', resp.text)
        
        results = []
        for p_attr, content in patterns:
            attrs = p_attr.split(',')
            video_time = float(attrs[0])
            date_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(attrs[4])))
            uid = attrs[6]
            results.append({
                'time': video_time,
                'date': date_time,
                'uid': uid,
                'content': content
            })
        return results
    except Exception as e:
        print(f"âŒ XML è§£æå¤±è´¥: {e}")
        return []

def save_danmaku_to_csv(danmaku_list, filename, is_first_write=False):
    headers = ['video_time', 'real_time', 'content', 'user_hash']
    
    with open(filename, mode='a', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        if is_first_write:
            writer.writerow(headers)
        
        count = 0
        for d in danmaku_list:
            writer.writerow([
                f"{d['time']:.2f}", 
                d['date'], 
                d['content'], 
                d['uid']
            ])
            count += 1
        return count

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("=======================================")
    print("     Bç«™è¯„è®º/å¼¹å¹• äº¤äº’å¼çˆ¬è™« v2.1     ")
    print("=======================================")
    
    input_bv = input("ğŸ“º è¯·è¾“å…¥è§†é¢‘ BV å· (ä¾‹å¦‚ BV1xx...): ").strip()
    if not input_bv:
        print("âš ï¸ æœªè¾“å…¥ BV å·ï¼Œå°†ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼...")
        input_bv = config.BV_CODE
    
    print(f"ğŸ” æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯: {input_bv} ...")
    video_info = get_video_info(input_bv)
    
    if not video_info:
        exit()
    
    print(f"âœ… è·å–æˆåŠŸ: {video_info['title']}")
    print(f"   UPä¸»: {video_info['owner']} | å‘å¸ƒäº: {video_info['pubdate']}")
    
    oid = video_info['oid']
    cid = video_info['cid']
    
    # 2. ç”¨æˆ·é€‰æ‹©
    print("\nè¯·é€‰æ‹©è¦çˆ¬å–çš„å†…å®¹ï¼š")
    print("1. ğŸ“ è¯„è®º (å« IPã€ç­‰çº§ã€ç‚¹èµç­‰)")
    print("2. ğŸš€ å¼¹å¹• (å½“å‰å¼¹å¹•æ± )")
    choice = input("ğŸ‘‰ è¯·è¾“å…¥æ•°å­— (1 æˆ– 2): ").strip()
    
    if choice == '1':
        page_input = input("\nğŸ“„ è¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•° (è¾“å…¥ -1 çˆ¬å–å…¨éƒ¨): ").strip()
        try:
            max_pages = int(page_input)
        except ValueError:
            max_pages = 5 
            print("âš ï¸ è¾“å…¥éæ³•ï¼Œé»˜è®¤çˆ¬å– 5 é¡µ")

        # 1. å†™å…¥å…ƒæ•°æ® (è¿™ä¼šè¦†ç›–æ—§æ–‡ä»¶)
        write_metadata(config.COMMENT_SAVE_PATH, video_info)
        print(f"ğŸ“ å·²å†™å…¥è§†é¢‘ä¿¡æ¯åˆ°: {config.COMMENT_SAVE_PATH}")

        print("\n--- å¼€å§‹çˆ¬å–è¯„è®º ---")
        total_saved = 0
        page = 1
        is_first_data = True # æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€æ‰¹æ•°æ®(ç”¨äºå†™è¡¨å¤´)
        
        while True:
            if max_pages != -1 and page > max_pages:
                print("âœ… è¾¾åˆ°æŒ‡å®šé¡µæ•°ï¼Œåœæ­¢ã€‚")
                break
            
            print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µ...", end="")
            replies = fetch_comments(oid, page)
            
            if not replies:
                print("\nâš ï¸ æœ¬é¡µæ— æ•°æ® (å¯èƒ½å·²çˆ¬å®Œ)ï¼Œåœæ­¢ã€‚")
                break
                
            saved_count = save_comments_to_csv(
                replies, 
                filename=config.COMMENT_SAVE_PATH, 
                is_first_write=is_first_data
            )
            
            # å†™å®Œä¸€æ¬¡åï¼Œåé¢å°±éƒ½ä¸æ˜¯â€œç¬¬ä¸€æ¬¡â€äº†
            if saved_count > 0:
                is_first_data = False
                
            total_saved += saved_count
            print(f" -> ä¿å­˜ {saved_count} æ¡")
            
            time.sleep(random.uniform(1.0, 2.5))
            page += 1
            
        print(f"\nğŸ‰ è¯„è®ºçˆ¬å–ç»“æŸï¼å…± {total_saved} æ¡ã€‚")

    elif choice == '2':
        # 1. å†™å…¥å…ƒæ•°æ®
        write_metadata(config.DANMAKU_SAVE_PATH, video_info)
        print(f"ğŸ“ å·²å†™å…¥è§†é¢‘ä¿¡æ¯åˆ°: {config.DANMAKU_SAVE_PATH}")
        
        print("\n--- å¼€å§‹çˆ¬å–å¼¹å¹• ---")
        danmaku_list = crawl_danmaku_xml(cid)
        
        if danmaku_list:
            count = save_danmaku_to_csv(
                danmaku_list, 
                filename=config.DANMAKU_SAVE_PATH, 
                is_first_write=True
            )
            print(f"\nğŸ‰ å¼¹å¹•çˆ¬å–ç»“æŸï¼å…± {count} æ¡ã€‚")
        else:
            print("âš ï¸ æœªçˆ¬å–åˆ°å¼¹å¹•ã€‚")
            
    else:
        print("âŒ è¾“å…¥æ— æ•ˆï¼Œç¨‹åºé€€å‡ºã€‚")