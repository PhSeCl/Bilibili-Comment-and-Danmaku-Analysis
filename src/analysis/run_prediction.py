import sys
import os
from pathlib import Path
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
import numpy as np
import re

# Add project root to sys.path
# This file is in src/analysis/, so PROJECT_ROOT is ../../
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

from src.utils import get_emotion_label

def run_prediction_pipeline(input_path=None, output_path=None, model_path=None, model=None, tokenizer=None):
    """
    ËøêË°åÈ¢ÑÊµãÊµÅÊ∞¥Á∫øÔºöËØªÂèñÊï∞ÊçÆ -> Âä†ËΩΩÊ®°Âûã -> È¢ÑÊµã -> ‰øùÂ≠òÁªìÊûú -> ËøîÂõû DataFrame
    
    Args:
        input_path: ËæìÂÖ• CSV Ë∑ØÂæÑ
        output_path: ËæìÂá∫ CSV Ë∑ØÂæÑ
        model_path: Ê®°ÂûãË∑ØÂæÑ (ÂèØÈÄâ)
        model: È¢ÑÂä†ËΩΩÁöÑÊ®°ÂûãÂØπË±° (ÂèØÈÄâÔºåÊé®Ëçê)
        tokenizer: È¢ÑÂä†ËΩΩÁöÑÂàÜËØçÂô®ÂØπË±° (ÂèØÈÄâÔºåÊé®Ëçê)
    """
    # 1. Ë∑ØÂæÑÂ§ÑÁêÜ
    if input_path is None:
        input_path = PROJECT_ROOT / "data" / "raw" / "comments.csv"
    else:
        input_path = Path(input_path)
        
    if output_path is None:
        # ÈªòËÆ§ËæìÂá∫Ë∑ØÂæÑÔºåÊ†πÊçÆËæìÂÖ•Êñá‰ª∂ÂêçËá™Âä®ÁîüÊàê
        output_path = PROJECT_ROOT / "data" / "processed" / f"{input_path.stem}_predicted.csv"
    else:
        output_path = Path(output_path)

    # 2. Ê®°ÂûãÂä†ËΩΩÈÄªËæë
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üíª Using device: {device}")

    # Â¶ÇÊûúÊ≤°Êúâ‰º†ÂÖ•È¢ÑÂä†ËΩΩÁöÑÊ®°ÂûãÔºåÂàôÂ∞ùËØïÂä†ËΩΩ
    if model is None or tokenizer is None:
        # Â∞ùËØï‰ªé src.analysis.model ÂØºÂÖ• (Â¶ÇÊûúÊú™ÊåáÂÆö model_path)
        if model_path is None:
            try:
                print("üöÄ Loading model from src.analysis.model configuration...")
                from src.analysis.model import model as loaded_model, tokenizer as loaded_tokenizer
                model = loaded_model
                tokenizer = loaded_tokenizer
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to import from src.analysis.model: {e}")
        
        # Â¶ÇÊûúÂØºÂÖ•Â§±Ë¥•ÊàñÊåáÂÆö‰∫Ü model_pathÔºåÂàôÊâãÂä®Âä†ËΩΩ
        if model is None:
            if model_path is None:
                LOCAL_MODEL_DIR = PROJECT_ROOT / "trained_models"
                HF_MODEL_ID = "ScarletShinku/bilibili-sentiment-bert"
                
                if LOCAL_MODEL_DIR.exists():
                    model_path = LOCAL_MODEL_DIR
                else:
                    model_path = HF_MODEL_ID
            
            print(f"üöÄ Loading model from: {model_path}")
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = AutoModelForSequenceClassification.from_pretrained(model_path)
            except Exception as e:
                print(f"‚ùå Failed to load model: {e}")
                return None
    
    # Á°Æ‰øùÊ®°ÂûãÂú®Ê≠£Á°ÆÁöÑËÆæÂ§á‰∏ä
    model = model.to(device)
                print(f"üöÄ Loading model from local directory: {model_path}")
            else:
                model_path = HF_MODEL_ID
                print(f"üöÄ Loading model from Hugging Face: {model_path}")
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)
        except Exception as e:
            print(f"‚ùå Failed to load model: {e}")
            return None

    # 3. ËØªÂèñÊï∞ÊçÆ
    print(f"üìñ Reading data from {input_path}...")
    try:
        # Â∞ùËØïËØªÂèñÔºåË∑≥ËøáÂèØËÉΩÁöÑÂùèË°å
        # Ëá™Âä®Ê£ÄÊµãË°®Â§¥ÔºöÂ¶ÇÊûúÁ¨¨‰∏ÄË°åÁúãËµ∑Êù•‰∏çÂÉèË°®Â§¥ÔºàÊØîÂ¶ÇÊòØÊ≥®ÈáäÔºâÔºåÂ∞ùËØïË∑≥Ëøá
        header_row = 0
        if input_path.exists():
            with open(input_path, 'r', encoding='utf-8-sig') as f:
                lines = f.readlines()
            for i, line in enumerate(lines):
                if line.strip() and not line.strip().startswith('#'):
                    header_row = i
                    break
        
        df = pd.read_csv(input_path, skiprows=header_row, encoding='utf-8-sig', on_bad_lines='skip')
        
        # Áªü‰∏ÄÂàóÂêçÔºöÁ°Æ‰øùÊúâ 'content' Âàó
        if 'content' not in df.columns:
            if 'message' in df.columns:
                df['content'] = df['message']
            elif 'text' in df.columns:
                df['content'] = df['text']
            
        if 'content' not in df.columns:
            print("‚ùå 'content' column not found in CSV.")
            print(f"Columns found: {df.columns.tolist()}")
            return None
            
        # Êï∞ÊçÆÊ∏ÖÊ¥ó
        df['content'] = df['content'].fillna("").astype(str)
        # ÁßªÈô§Á©∫ÁôΩÂÜÖÂÆπ
        df = df[df['content'].str.strip() != ""]
        # ÁßªÈô§ "ÂõûÂ§ç @xxx :" Êàñ "@xxx :" (ÂÖºÂÆπ‰∏çÂêåÊ†ºÂºè)
        # Ê≠£ÂàôËß£Èáä: ^(?:ÂõûÂ§ç\s*)? ÂåπÈÖçÂºÄÂ§¥ÂèØÈÄâÁöÑ"ÂõûÂ§ç"ÂíåÁ©∫Ê†º
        # @.*? ÂåπÈÖç @Áî®Êà∑Âêç (ÈùûË¥™Â©™)
        # [Ôºö:]\s* ÂåπÈÖç‰∏≠Ëã±ÊñáÂÜíÂè∑ÂíåÂêéÁª≠Á©∫Ê†º
        df["content"] = df["content"].apply(lambda x: re.sub(r'^(?:ÂõûÂ§ç\s*)?@.*?[Ôºö:]\s*', '', x).strip())
        df = df[df["content"] != ""]
        
        print(f"üìä Total items to analyze: {len(df)}")
        
    except Exception as e:
        print(f"‚ùå Failed to read data: {e}")
        return None

    # 4. ÊâßË°åÈ¢ÑÊµã
    print("üîÆ Running inference...")
    batch_size = 32
    predictions = []
    
    model.eval()
    
    # ÊâπÈáèÈ¢ÑÊµã
    texts = df['content'].tolist()
    for i in tqdm(range(0, len(texts), batch_size), desc="Predicting"):
        batch_texts = texts[i : i + batch_size]
        
        inputs = tokenizer(
            batch_texts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=128
        ).to(device)
        
        with torch.no_grad():
            logits = model(**inputs).logits
            preds = torch.argmax(logits, dim=-1).cpu().numpy()
            predictions.extend(preds)
            
    df['predicted_label_id'] = predictions
    # Ëé∑Âèñ‰∏≠ÊñáÊÉÖÊÑüÊ†áÁ≠æ
    df['predicted_emotion'] = df['predicted_label_id'].apply(lambda x: get_emotion_label(x, use_zh=True))
    
    # ÂÖºÂÆπÊóß‰ª£Á†ÅÔºåÂèØËÉΩÈúÄË¶Å 'labels' Âàó
    df['labels'] = predictions

    # 5. ‰øùÂ≠òÁªìÊûú
    output_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"üíæ Saving predictions to {output_path}")
    # ‰ΩøÁî® mode='w' Ë¶ÜÁõñÂÜôÂÖ•
    df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='w')
    
    return df

def main():
    """
    CLI ÂÖ•Âè£ÂáΩÊï∞
    """
    print("========================================")
    print("   Bilibili ÊÉÖÊÑüÂàÜÊûê - ‰∫§‰∫íÂºèÈ¢ÑÊµãÂ∑•ÂÖ∑")
    print("========================================")
    print("ËØ∑ÈÄâÊã©Ë¶ÅÈ¢ÑÊµãÁöÑÊï∞ÊçÆÁ±ªÂûã:")
    print("1. comments (ËØÑËÆ∫)")
    print("2. danmaku (ÂºπÂπï)")
    
    while True:
        choice = input("ËØ∑ËæìÂÖ•ÊÇ®ÁöÑÈÄâÊã© (ËæìÂÖ• comments Êàñ danmaku): ").strip().lower()
        
        if choice in ['1', 'comments', 'comment']:
            data_type = 'comment'
            break
        elif choice in ['2', 'danmaku']:
            data_type = 'danmaku'
            break
        else:
            print("‚ùå ËæìÂÖ•Êó†ÊïàÔºåËØ∑ËæìÂÖ• 'comments' Êàñ 'danmaku'")

    # ÈÖçÁΩÆË∑ØÂæÑ
    RAW_DIR = PROJECT_ROOT / "data" / "raw"
    PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
    
    if data_type == "comment":
        INPUT_FILE = RAW_DIR / "comments.csv"
        OUTPUT_FILE = PROCESSED_DIR / "comments_predicted.csv"
    else:
        INPUT_FILE = RAW_DIR / "danmaku.csv"
        OUTPUT_FILE = PROCESSED_DIR / "danmaku_predicted.csv"

    # Ê£ÄÊü•ËæìÂÖ•Êñá‰ª∂
    if not INPUT_FILE.exists():
        print(f"‚ùå Ê≤°ÊúâÊâæÂà∞ÂØπÂ∫îÊñá‰ª∂: {INPUT_FILE}")
        print("ËØ∑ÂÖàËøêË°åÁà¨Ëô´ËøõË°åÁà¨Âèñ")
        return

    # Ë∞ÉÁî®ÊµÅÊ∞¥Á∫øÂáΩÊï∞
    df = run_prediction_pipeline(input_path=INPUT_FILE, output_path=OUTPUT_FILE)
    
    if df is not None:
        print(f"‚úÖ È¢ÑÊµãÂÆåÊàêÔºÅÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: {OUTPUT_FILE}")
        print("üëÄ È¢ÑËßàÂâç 5 Êù°ÁªìÊûú:")
        print(df[['content', 'predicted_emotion']].head())

if __name__ == "__main__":
    main()

# Try importing visualization module
try:
    from src.visualization.distribution import plot_emotion_distribution, print_emotion_statistics
except ImportError:
    print("‚ö†Ô∏è Could not import visualization modules. Please ensure src is a package.")

def run_prediction_pipeline(input_path=None, output_path=None, model_path=None):
    """
    ËøêË°åÈ¢ÑÊµãÊµÅÊ∞¥Á∫øÔºöËØªÂèñÊï∞ÊçÆ -> Âä†ËΩΩÊ®°Âûã -> È¢ÑÊµã -> ‰øùÂ≠òÁªìÊûú -> ËøîÂõû DataFrame
    """
    # Paths
    if input_path is None:
        input_path = PROJECT_ROOT / "data" / "raw" / "comments.csv"
    else:
        input_path = Path(input_path)
        
    if output_path is None:
        output_path = PROJECT_ROOT / "data" / "processed" / "comments_with_predictions.csv"
    else:
        output_path = Path(output_path)

    # Model Path Logic
    if model_path is None:
        LOCAL_MODEL_DIR = PROJECT_ROOT / "trained_models"
        HF_MODEL_ID = "ScarletShinku/bilibili-sentiment-bert"
        
        if LOCAL_MODEL_DIR.exists():
            model_path = LOCAL_MODEL_DIR
            print(f"üöÄ Loading model from local directory: {model_path}")
        else:
            model_path = HF_MODEL_ID
            print(f"üöÄ Loading model from Hugging Face: {model_path}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return None

    print(f"üìñ Reading data from {input_path}...")
    try:
        # Try to find the header row if it's not the first one
        header_row = 0
        if input_path.exists():
            with open(input_path, 'r', encoding='utf-8-sig') as f:
                lines = f.readlines()
            for i, line in enumerate(lines):
                if line.strip() and not line.strip().startswith('#'):
                    header_row = i
                    break
        
        df = pd.read_csv(input_path, skiprows=header_row)
        
        # Check for content column
        if 'content' not in df.columns and 'message' in df.columns:
             df['content'] = df['message']
             
        if 'content' not in df.columns:
            print("‚ùå 'content' column not found in CSV.")
            print(f"Columns found: {df.columns.tolist()}")
            return None
            
        # Clean data
        df['content'] = df['content'].fillna("").astype(str)
        df = df[df['content'].str.strip() != ""]
        
        # Remove "ÂõûÂ§ç @xxx :"
        df["content"] = df["content"].apply(lambda x: re.sub(r'^ÂõûÂ§ç @.*? :', '', x).strip())
        df = df[df["content"] != ""]
        
        print(f"üìä Total comments to analyze: {len(df)}")
        
    except Exception as e:
        print(f"‚ùå Failed to read data: {e}")
        return None

    # Inference
    print("üîÆ Running inference...")
    batch_size = 32
    predictions = []
    
    model.eval()
    
    # Process in batches
    for i in tqdm(range(0, len(df), batch_size)):
        batch_texts = df['content'].iloc[i:i+batch_size].tolist()
        
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
        
        with torch.no_grad():
            logits = model(**inputs).logits
            preds = torch.argmax(logits, dim=-1).cpu().numpy()
            predictions.extend(preds)
            
    df['labels'] = predictions
    
    # Save results
    output_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"üíæ Saved predictions to {output_path}")
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    return df

def main():
    df = run_prediction_pipeline()
    
    if df is not None:
        # Visualization
        print("üé® Generating visualization...")
        try:
            plot_emotion_distribution(df, save_path=PROJECT_ROOT / "docs" / "images" / "emotion_distribution_pie_bar.png")
            print_emotion_statistics(df)
        except Exception as e:
            print(f"‚ö†Ô∏è Visualization failed: {e}")

if __name__ == "__main__":
    main()