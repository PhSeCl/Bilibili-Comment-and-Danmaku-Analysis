import sys
import os
from pathlib import Path
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
import numpy as np
import re

# Add project root to sys.path
# This file is in src/analysis/, so PROJECT_ROOT is ../../
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

from src.utils import get_emotion_label

def run_prediction_pipeline(input_path=None, output_path=None, model_path=None, model=None, tokenizer=None):
    """
    è¿è¡Œé¢„æµ‹æµæ°´çº¿ï¼šè¯»å–æ•°æ® -> åŠ è½½æ¨¡å‹ -> é¢„æµ‹ -> ä¿å­˜ç»“æœ -> è¿”å› DataFrame
    
    Args:
        input_path: è¾“å…¥ CSV è·¯å¾„
        output_path: è¾“å‡º CSV è·¯å¾„
        model_path: æ¨¡å‹è·¯å¾„ (å¯é€‰)
        model: é¢„åŠ è½½çš„æ¨¡å‹å¯¹è±¡ (å¯é€‰ï¼Œæ¨è)
        tokenizer: é¢„åŠ è½½çš„åˆ†è¯å™¨å¯¹è±¡ (å¯é€‰ï¼Œæ¨è)
    """
    # 1. è·¯å¾„å¤„ç†
    if input_path is None:
        input_path = PROJECT_ROOT / "data" / "raw" / "comments.csv"
    else:
        input_path = Path(input_path)
        
    if output_path is None:
        # é»˜è®¤è¾“å‡ºè·¯å¾„ï¼Œæ ¹æ®è¾“å…¥æ–‡ä»¶åè‡ªåŠ¨ç”Ÿæˆ
        output_path = PROJECT_ROOT / "data" / "processed" / f"{input_path.stem}_predicted.csv"
    else:
        output_path = Path(output_path)

    # 2. æ¨¡å‹åŠ è½½é€»è¾‘
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» Using device: {device}")

    # å¦‚æœæ²¡æœ‰ä¼ å…¥é¢„åŠ è½½çš„æ¨¡å‹ï¼Œåˆ™å°è¯•åŠ è½½
    if model is None or tokenizer is None:
        # å°è¯•ä» src.analysis.model å¯¼å…¥ (å¦‚æœæœªæŒ‡å®š model_path)
        if model_path is None:
            try:
                print("ğŸš€ Loading model from src.analysis.model configuration...")
                from src.analysis.model import model as loaded_model, tokenizer as loaded_tokenizer
                model = loaded_model
                tokenizer = loaded_tokenizer
            except Exception as e:
                print(f"âš ï¸ Failed to import from src.analysis.model: {e}")
        
        # å¦‚æœå¯¼å…¥å¤±è´¥æˆ–æŒ‡å®šäº† model_pathï¼Œåˆ™æ‰‹åŠ¨åŠ è½½
        if model is None:
            if model_path is None:
                LOCAL_MODEL_DIR = PROJECT_ROOT / "trained_models"
                HF_MODEL_ID = "ScarletShinku/bilibili-sentiment-bert"
                
                if LOCAL_MODEL_DIR.exists():
                    model_path = LOCAL_MODEL_DIR
                else:
                    model_path = HF_MODEL_ID
            
            print(f"ğŸš€ Loading model from: {model_path}")
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = AutoModelForSequenceClassification.from_pretrained(model_path)
            except Exception as e:
                print(f"âŒ Failed to load model: {e}")
                return None
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    model = model.to(device)

    # 3. è¯»å–æ•°æ®
    print(f"ğŸ“– Reading data from {input_path}...")
    try:
        # å°è¯•è¯»å–ï¼Œè·³è¿‡å¯èƒ½çš„åè¡Œ
        # è‡ªåŠ¨æ£€æµ‹è¡¨å¤´ï¼šå¦‚æœç¬¬ä¸€è¡Œçœ‹èµ·æ¥ä¸åƒè¡¨å¤´ï¼ˆæ¯”å¦‚æ˜¯æ³¨é‡Šï¼‰ï¼Œå°è¯•è·³è¿‡
        header_row = 0
        if input_path.exists():
            with open(input_path, 'r', encoding='utf-8-sig') as f:
                lines = f.readlines()
            for i, line in enumerate(lines):
                if line.strip() and not line.strip().startswith('#'):
                    header_row = i
                    break
        
        df = pd.read_csv(input_path, skiprows=header_row, encoding='utf-8-sig', on_bad_lines='skip')
        
        # ç»Ÿä¸€åˆ—åï¼šç¡®ä¿æœ‰ 'content' åˆ—
        if 'content' not in df.columns:
            if 'message' in df.columns:
                df['content'] = df['message']
            elif 'text' in df.columns:
                df['content'] = df['text']
            
        if 'content' not in df.columns:
            print("âŒ 'content' column not found in CSV.")
            print(f"Columns found: {df.columns.tolist()}")
            return None
            
        # æ•°æ®æ¸…æ´—
        df['content'] = df['content'].fillna("").astype(str)
        # ç§»é™¤ç©ºç™½å†…å®¹
        df = df[df['content'].str.strip() != ""]
        # ç§»é™¤ "å›å¤ @xxx :" æˆ– "@xxx :" (å…¼å®¹ä¸åŒæ ¼å¼)
        # æ­£åˆ™è§£é‡Š: ^(?:å›å¤\s*)? åŒ¹é…å¼€å¤´å¯é€‰çš„"å›å¤"å’Œç©ºæ ¼
        # @.*? åŒ¹é… @ç”¨æˆ·å (éè´ªå©ª)
        # [ï¼š:]\s* åŒ¹é…ä¸­è‹±æ–‡å†’å·å’Œåç»­ç©ºæ ¼
        df["content"] = df["content"].apply(lambda x: re.sub(r'^(?:å›å¤\s*)?@.*?[ï¼š:]\s*', '', x).strip())
        df = df[df["content"] != ""]
        
        print(f"ğŸ“Š Total items to analyze: {len(df)}")
        
    except Exception as e:
        print(f"âŒ Failed to read data: {e}")
        return None

    # 4. æ‰§è¡Œé¢„æµ‹
    print("ğŸ”® Running inference...")
    batch_size = 32
    predictions = []
    
    model.eval()
    
    # æ‰¹é‡é¢„æµ‹
    texts = df['content'].tolist()
    for i in tqdm(range(0, len(texts), batch_size), desc="Predicting"):
        batch_texts = texts[i : i + batch_size]
        
        inputs = tokenizer(
            batch_texts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=128
        ).to(device)
        
        with torch.no_grad():
            logits = model(**inputs).logits
            preds = torch.argmax(logits, dim=-1).cpu().numpy()
            predictions.extend(preds)
            
    df['predicted_label_id'] = predictions
    # è·å–ä¸­æ–‡æƒ…æ„Ÿæ ‡ç­¾
    df['predicted_emotion'] = df['predicted_label_id'].apply(lambda x: get_emotion_label(x, use_zh=True))
    
    # å…¼å®¹æ—§ä»£ç ï¼Œå¯èƒ½éœ€è¦ 'labels' åˆ—
    df['labels'] = predictions

    # 5. ä¿å­˜ç»“æœ
    output_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ Saving predictions to {output_path}")
    # ä½¿ç”¨ mode='w' è¦†ç›–å†™å…¥
    df.to_csv(output_path, index=False, encoding='utf-8-sig', mode='w')
    
    return df

def main():
    """
    CLI å…¥å£å‡½æ•°
    """
    print("========================================")
    print("   Bilibili æƒ…æ„Ÿåˆ†æ - äº¤äº’å¼é¢„æµ‹å·¥å…·")
    print("========================================")
    print("è¯·é€‰æ‹©è¦é¢„æµ‹çš„æ•°æ®ç±»å‹:")
    print("1. comments (è¯„è®º)")
    print("2. danmaku (å¼¹å¹•)")
    
    while True:
        choice = input("è¯·è¾“å…¥æ‚¨çš„é€‰æ‹© (è¾“å…¥ comments æˆ– danmaku): ").strip().lower()
        
        if choice in ['1', 'comments', 'comment']:
            data_type = 'comment'
            break
        elif choice in ['2', 'danmaku']:
            data_type = 'danmaku'
            break
        else:
            print("âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ 'comments' æˆ– 'danmaku'")

    # é…ç½®è·¯å¾„
    RAW_DIR = PROJECT_ROOT / "data" / "raw"
    PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
    
    if data_type == "comment":
        INPUT_FILE = RAW_DIR / "comments.csv"
        OUTPUT_FILE = PROCESSED_DIR / "comments_predicted.csv"
    else:
        INPUT_FILE = RAW_DIR / "danmaku.csv"
        OUTPUT_FILE = PROCESSED_DIR / "danmaku_predicted.csv"

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not INPUT_FILE.exists():
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯¹åº”æ–‡ä»¶: {INPUT_FILE}")
        print("è¯·å…ˆè¿è¡Œçˆ¬è™«è¿›è¡Œçˆ¬å–")
        return

    # è°ƒç”¨æµæ°´çº¿å‡½æ•°
    df = run_prediction_pipeline(input_path=INPUT_FILE, output_path=OUTPUT_FILE)
    
    if df is not None:
        print(f"âœ… é¢„æµ‹å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")
        print("ğŸ‘€ é¢„è§ˆå‰ 5 æ¡ç»“æœ:")
        print(df[['content', 'predicted_emotion']].head())

if __name__ == "__main__":
    main()

