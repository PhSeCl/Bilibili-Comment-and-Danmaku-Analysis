import os
import json
import argparse  # æ–°å¢ï¼šå‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer

# ============ é»˜è®¤é…ç½®åŒº ============
# è‡ªåŠ¨æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
DATA_RAW_DIR = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"

DEFAULT_MODEL_ID = "hfl/chinese-roberta-wwm-ext"
DEFAULT_MAX_LEN = 128
OUTPUT_BASE_DIR = str(DATA_PROCESSED_DIR)
TEST_SIZE = 0.2
RANDOM_STATE = 42
# ===================================

def parse_args():
    parser = argparse.ArgumentParser(description="æ•°æ®é¢„å¤„ç†è„šæœ¬")
    parser.add_argument("--input", type=str, default=str(DATA_RAW_DIR / "comments.csv"), 
                        help="åŸå§‹ CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--type", type=str, default="comment", choices=["comment", "danmaku"], 
                        help="æ•°æ®ç±»å‹: comment æˆ– danmaku")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL_ID, help="HuggingFace æ¨¡å‹ ID")
    parser.add_argument("--num_labels", type=int, default=8, help="åˆ†ç±»æ ‡ç­¾æ•°")
    return parser.parse_args()

def detect_header_row(filepath):
    """è‡ªåŠ¨å¯»æ‰¾ CSV çš„è¡¨å¤´è¡Œï¼ˆè·³è¿‡ # å¼€å¤´çš„å…ƒæ•°æ®ï¼‰"""
    with open(filepath, 'r', encoding='utf-8-sig') as f:
        lines = f.readlines()
    
    for i, line in enumerate(lines):
        # æ‰¾åˆ°ç¬¬ä¸€è¡Œä¸ä»¥ # å¼€å¤´ä¸”éç©ºçš„è¡Œï¼Œè¿™å°±æ˜¯è¡¨å¤´
        if line.strip() and not line.strip().startswith('#'):
            return i
    return 0

def parse_time(s: str):
    """å¢å¼ºç‰ˆæ—¶é—´è§£æï¼Œé€‚é… B ç«™çˆ¬è™«æ ¼å¼"""
    s = str(s).strip()
    if not s or s.lower() == 'nan':
        return {"hour": -1, "weekday": -1}
    
    # å¸¸è§æ ¼å¼åˆ—è¡¨
    formats = [
        "%Y-%m-%d %H:%M:%S", # çˆ¬è™«æ ‡å‡†æ ¼å¼ 2025-01-01 12:00:00
        "%Y/%m/%d %H:%M", 
        "%Y-%m-%d %H:%M", 
        "%Y/%m/%d"
    ]
    
    for fmt in formats:
        try:
            dt = datetime.strptime(s, fmt)
            return {"hour": dt.hour, "weekday": dt.weekday()}
        except ValueError:
            continue
            
    # å…œåº•ï¼šå¦‚æœåªæ˜¯æ—¥æœŸæ²¡æœ‰æ—¶é—´ï¼Œæˆ–è€…æ ¼å¼å¤ªæ€ª
    return {"hour": -1, "weekday": -1}

def main():
    args = parse_args()
    NUM_LABELS = args.num_labels  # è·å–æ ‡ç­¾æ•°
    
    print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†: {args.input} (ç±»å‹: {args.type})")
    
    if not os.path.exists(args.input):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    # 1. æ™ºèƒ½è¯»å– CSV
    header_row = detect_header_row(args.input)
    print(f"ğŸ” æ£€æµ‹åˆ°è¡¨å¤´åœ¨ç¬¬ {header_row} è¡Œ")
    
    df = pd.read_csv(args.input, skiprows=header_row, dtype=str, keep_default_na=False, na_values=["", "NA", "NaN"])
    print(f"ğŸ“Š åŸå§‹æ•°æ®é‡: {len(df)} æ¡")

    # 2. åˆ—åæ ‡å‡†åŒ– (æ ¹æ®æ•°æ®ç±»å‹ä¸åŒ)
    # åˆ›å»ºä¸€ä¸ªæ–°çš„æ ‡å‡† DataFrame
    new_df = pd.DataFrame()
    
    # é€šç”¨åˆ—
    if 'content' in df.columns:
        new_df['content'] = df['content']
    elif 'message' in df.columns:
        new_df['content'] = df['message']
    else:
        print("âŒ æ‰¾ä¸åˆ° content åˆ—ï¼è¯·æ£€æŸ¥ CSV è¡¨å¤´ã€‚")
        return

    # å¤„ç†æ ‡ç­¾åˆ—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    if 'label' in df.columns:
        new_df['label'] = df['label'].astype(int)
    else:
        print("âš ï¸  æœªæ‰¾åˆ° label åˆ—ï¼Œä½¿ç”¨éšæœºæ ‡ç­¾è¿›è¡Œæ¼”ç¤º")
        new_df['label'] = np.random.randint(0, NUM_LABELS, len(df))

    # ç±»å‹ç‰¹å®šåˆ—å¤„ç†
    if args.type == 'comment':
        # è¯„è®ºæ•°æ®æ˜ å°„
        new_df['time'] = df['date'] if 'date' in df.columns else ""
        new_df['location'] = df['location'] if 'location' in df.columns else "unknown"
        new_df['user_level'] = df['level'] if 'level' in df.columns else "0"
        new_df['likes'] = df['likes'] if 'likes' in df.columns else "0"
        new_df['username'] = df['username'] if 'username' in df.columns else "unknown"
        
    elif args.type == 'danmaku':
        # å¼¹å¹•æ•°æ®æ˜ å°„
        new_df['time'] = df['real_time'] if 'real_time' in df.columns else ""
        # å¼¹å¹•æ²¡æœ‰ location, likes, levelï¼Œç»™é»˜è®¤å€¼
        new_df['location'] = "unknown"
        new_df['user_level'] = "0"
        new_df['likes'] = "0"
        new_df['username'] = df['user_hash'] if 'user_hash' in df.columns else "unknown"

    # 3. åŸºç¡€æ¸…æ´—
    # å»é™¤ç©ºå†…å®¹
    new_df["content"] = new_df["content"].fillna("").astype(str)
    new_df = new_df[new_df["content"].str.strip() != ""].reset_index(drop=True)
    
    # å»é™¤ "å›å¤ @xxx :" (è¿™å¯¹æƒ…æ„Ÿåˆ†æå¾ˆé‡è¦)
    import re
    new_df["content"] = new_df["content"].apply(lambda x: re.sub(r'^å›å¤ @.*? :', '', x).strip())
    new_df = new_df[new_df["content"] != ""] # å†æ¬¡æ¸…æ´—å¯èƒ½å˜ç©ºçš„è¡Œ

    print(f"ğŸ§¹ æ¸…æ´—åæ•°æ®é‡: {len(new_df)} æ¡")

    # 4. ç‰¹å¾å·¥ç¨‹
    # A. æ—¶é—´ç‰¹å¾
    parsed = new_df["time"].apply(parse_time)
    new_df["hour"] = parsed.apply(lambda x: x["hour"])
    new_df["weekday"] = parsed.apply(lambda x: x["weekday"])

    # B. åœ°ç‚¹ç‰¹å¾ (Mapping)
    new_df["location"] = new_df["location"].fillna("").replace("", "unknown")
    # ç®€å•æ¸…æ´—ï¼šå»é™¤ "IPå±åœ°ï¼š"
    new_df["location"] = new_df["location"].apply(lambda x: str(x).replace("IPå±åœ°ï¼š", ""))
    
    unique_locs = new_df["location"].unique().tolist()
    loc2id = {loc: idx for idx, loc in enumerate(unique_locs)}
    new_df["loc_id"] = new_df["location"].map(loc2id).fillna(-1).astype(int)

    # ä¿å­˜ loc æ˜ å°„
    loc_map_path = os.path.join(OUTPUT_BASE_DIR, f"{args.type}_loc2id.json")
    os.makedirs(os.path.dirname(loc_map_path), exist_ok=True)
    with open(loc_map_path, "w", encoding="utf-8") as f:
        json.dump(loc2id, f, ensure_ascii=False, indent=2)

    # C. ç”¨æˆ·ç­‰çº§ & ç‚¹èµ (æ•°å€¼åŒ– + æ ‡å‡†åŒ–)
    def safe_float(x):
        try:
            return float(x)
        except:
            return 0.0
            
    new_df["user_level_num"] = new_df["user_level"].apply(safe_float)
    new_df["likes_num"] = new_df["likes"].apply(safe_float)
    
    # Z-Score æ ‡å‡†åŒ– (é˜²æ­¢é™¤ä»¥0)
    def standardize(series):
        std = series.std()
        if std == 0: return series - series.mean()
        return (series - series.mean()) / std

    new_df["user_level_norm"] = standardize(new_df["user_level_num"])
    # ç‚¹èµæ•°é•¿å°¾åˆ†å¸ƒï¼Œå…ˆ log å†å½’ä¸€åŒ–
    new_df["likes_log"] = np.log1p(new_df["likes_num"]) 
    new_df["likes_norm"] = standardize(new_df["likes_log"])

    # 5. æ„é€  extra å‘é‡ [hour, weekday, loc_id, level, likes]
    # æ³¨æ„ï¼šBERT æ¨¡å‹åªæ¥å— Tensorï¼Œæ‰€ä»¥æ‰€æœ‰å€¼å¿…é¡»æ˜¯ float/int
    def make_extra(row):
        return [
            float(row["hour"]), 
            float(row["weekday"]), 
            float(row["loc_id"]),
            float(row["user_level_norm"]), 
            float(row["likes_norm"])
        ]

    new_df["extra"] = new_df.apply(make_extra, axis=1)

    # 6. åˆ’åˆ†æ•°æ®é›† (Train/Val)
    # åªæœ‰æ•°æ®é‡è¶³å¤Ÿå¤§æ—¶æ‰åˆ’åˆ†ï¼Œå¦åˆ™å…¨é‡ä½œä¸º train
    if len(new_df) > 10:
        train_df, val_df = train_test_split(new_df, test_size=TEST_SIZE, random_state=RANDOM_STATE)
    else:
        train_df, val_df = new_df, new_df.iloc[:0] # ç©ºçš„ val
        
    train_df = train_df.reset_index(drop=True)
    val_df = val_df.reset_index(drop=True)

    # 7. è½¬ä¸º HF Dataset å¹¶ Tokenize
    ds_train = Dataset.from_pandas(train_df[["content", "extra", "username", "label"]])
    ds_val = Dataset.from_pandas(val_df[["content", "extra", "username", "label"]])
    ds = DatasetDict({"train": ds_train, "validation": ds_val})

    print("â³ æ­£åœ¨ Tokenize (ä½¿ç”¨æ¨¡å‹: {})...".format(args.model))
    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)

    def tokenize_fn(examples):
        out = tokenizer(examples["content"], truncation=True, max_length=DEFAULT_MAX_LEN)
        out["extra"] = examples["extra"]
        out["labels"] = examples["label"]  # è½¬æ¢ä¸º "labels"ï¼ˆæ¨¡å‹è®­ç»ƒéœ€è¦ï¼‰
        out["username"] = examples.get("username", [""] * len(examples["content"]))
        return out

    remove_cols = ["content", "extra", "username", "label"]
    # æŸäº›ç‰ˆæœ¬ datasets å¯èƒ½éœ€è¦ remove_columns å‚æ•°æ¥æ¸…é™¤åŸå§‹æ–‡æœ¬åˆ—ä»¥èŠ‚çœç©ºé—´
    tokenized = ds.map(tokenize_fn, batched=True, remove_columns=remove_cols)

    # 8. ä¿å­˜
    save_path = os.path.join(OUTPUT_BASE_DIR, f"{args.type}_tokenized_dataset")
    tokenized.save_to_disk(save_path)
    
    print("âœ… é¢„å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ Dataset ä¿å­˜è·¯å¾„: {save_path}")
    print(f"ğŸ“ Location æ˜ å°„è·¯å¾„: {loc_map_path}")
    print(f"ğŸ“Š è®­ç»ƒé›†æ ·æœ¬æ•°: {len(tokenized['train'])}")

if __name__ == "__main__":
    main()